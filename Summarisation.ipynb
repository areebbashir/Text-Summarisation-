{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b5f620b-77d6-465b-96fd-304f38bef380",
   "metadata": {},
   "source": [
    "Installing Metaphor library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b8fca0-53db-44b3-9f3c-25efe70bf69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install metaphor-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18eaa5-8e42-48d1-9690-9b7c84af2d82",
   "metadata": {},
   "source": [
    "# Text Summarization \n",
    "Text Summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n",
    "\n",
    "### WikiHow\n",
    "is a new large-scale dataset using the online WikiHow (http://www.wikihow.com) knowledge base. It contains ~2000 articles which have title, text, and headlines(summary).\n",
    "\n",
    "I have created Attention Based GRU model along with the implementation of teacher forcing and dot product attention mechanism.\n",
    "\n",
    "After training the dataset for 150000 steps I tested the dataset that I created from the text I obtained using METAPHOR's api. I have stored the text and its predicted summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6a727d-4011-43ee-a0b0-db4eaaf199da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing wikiHow dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"wikihowAll.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c878fe8b-af42-4035-a5ce-60ed82bee45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbacfa45-9b3b-4f44-a4c9-9334392d0ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07796be2-2559-4584-8a17-a45db3e54481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215365, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98159ffd-3179-4245-810f-31c9d0de40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['text'].isnull()==False]\n",
    "df = df[df['headline'].isnull()==False]\n",
    "df = df[df['text']!=\"nan\"]\n",
    "df = df[df['headline']!=\"nan\"]\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8eec5772-debf-4738-a3a2-576f4b4e0599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214294, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98f8128f-473f-4e2e-878e-c87e4c8af23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "429b457b-ac32-43c0-80aa-bbae10cbb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(get_stop_words('en')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "  str = text.lower()\n",
    "  str = BeautifulSoup(str, features=\"lxml\").text\n",
    "  str = re.sub(r'\\([^)]*\\)', '', str)\n",
    "  str = re.sub('\"','', str)\n",
    "  str = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in str.split(\" \")])    \n",
    "  str = re.sub(r\"'s\\b\",\"\",str)\n",
    "  str = re.sub(\"[^a-zA-Z]\", \" \", str) \n",
    "  str = re.sub('[m]{2,}', 'mm', str)\n",
    "  if(num==0):\n",
    "    str = re.sub(r'\\.',' . ',str)\n",
    "  if(num==0):\n",
    "      tokens = [w for w in str.split() if not w in stop_words]\n",
    "      \n",
    "  else:\n",
    "      tokens=str.split()\n",
    "  long_words=[]\n",
    "  for i in tokens:\n",
    "      if len(i)>1:          #removing short words\n",
    "          long_words.append(i)\n",
    "  return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9195e9-2248-4580-b666-88f315cfaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f0ce92e-2f31-4f8c-8508-040f666409f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the dataset\n",
    "clean_text = []\n",
    "for t in df['text']:\n",
    "    clean_text.append(text_cleaner(t,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e08530-ba32-4ae2-8a7e-8ac628c66706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the dataset\n",
    "clean_summary = []\n",
    "for t in df['headline']:\n",
    "    clean_summary.append(text_cleaner(t,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1f49a63-2b58-4aee-858d-43e73e9f3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=clean_text\n",
    "df['headline']=clean_summary\n",
    "# Droping the null rows\n",
    "df.replace('', np.nan, inplace=True)\n",
    "df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd650b16-082c-488c-b03d-691d41c84d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the graph\n",
    "# We can fix maximum length of text = 150 since most of the reviews have a length of 150 and maximum headline length of 50, since maximum headlines are of size 50\n",
    "max_len_text= 150\n",
    "max_len_headline=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9522fb2-e4a1-45fd-9424-aabeefeff797",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 =np.array(df['text'])\n",
    "headline1=np.array(df['headline'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(text1)):\n",
    "    if(len(headline1[i].split())<=max_len_text and len(text1[i].split())<=max_len_headline):\n",
    "        short_text.append(text1[i])\n",
    "        short_summary.append(headline1[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad04ca02-7cfa-426d-bc7e-4841c1dd2bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even just split seconds will put guy little pickle asked likes girl even know try friend looking knock one books desk right front guy step away quickly jerk will pick now will know talking ask likes friend stalk showering questions will probably little freaked turn lunch go sit table friends one ask reply give time ask next week might little embarrassed asked front friends might laugh something ask see private make sure sound casual otherwise will think like guys will say know maybe gotta think press much just try get definite answer says yes tell everyone school humongous turn knows trying set two doesn say anything even know setting make sure goes talks arrange date humbly take million thank yous gives\n",
      "get friend dream guy notice casual try ask private yes don yell everyone says tell friend says yes tell friend\n"
     ]
    }
   ],
   "source": [
    "print(df['text'][50],df['summary'][50],sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34d05697-3c85-475e-95a7-bb004c891dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(df['text'],df['summary'],test_size=0.2,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bf6a888-d704-4f20-b569-d5337b1573cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65369\n",
      "16343\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd5b8720-efe1-44cf-b08f-4aed8012c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05eed70c-d69e-4988-b315-b6bdcc02012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05f83175-99bd-47ad-ad01-1c9d67373360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(text, summary, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    text=np.array(text)\n",
    "    summary=np.array(summary)\n",
    "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(summary)\n",
    "        output_lang = Lang(text)\n",
    "    else:\n",
    "        input_lang = Lang(text)\n",
    "        output_lang = Lang(summary)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3c02a06-7bc8-4a15-bdc3-19ff91fafefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b87e7a60-8955-44d0-a631-a6d96eecfdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 65369 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "['want break scary illusion atmosphere either play day keep bright lights night sound basically half scariness atmosphere game read faq guide panic fighting solving puzzles watch videos game footage can expect coming generally merrier will makes feel better calm'\n",
      " 'can find settings app one home screens may folder labeled utilities will see fifth group options account set default notes account notes created siri will added account notes displayed notes widget today view will account well can add additional accounts iphone support notes will able sync notes notes app can find fifth group options will see list accounts already added iphone previously added account enable notes can tap enable notes switch will display list accounts can add account listed tap add manually accounts will just need enter email address password iphone will attempt connect account using login information provided switch enabled will able store sync notes account can tell switch turned moves right turns green account will added iphone will able select default notes account'\n",
      " 'put separate hangers one wardrobe bag put order show keep accessories costumes get durable large bag lots internal pockets will able find things faster know alternatively bring plastic folder bits revision can look will never hang around nothing hour always keep basics bag dance shoes extra socks make removing wipes regular wipes bobby pins scrunchies hair nets hairbrush hairspray hairbands bandaids toe tape small medical kit water bottle food allowed keep dried fruit bananas granola bars bag times get hungry clear nail polish get small run tights coat clear nail polish let dry will stop run tracks need fancy cheap ones work fine also good bring sewing kit case leotard tears ribbons pointe shoes come'\n",
      " ...\n",
      " 'separated cards two decks shuffle deck separately give card black deck boys give card form red deck girls don look cards hand hand random everyone hold cards front everyone can see card number face ask everyone look person whose card number face matches matching numbers hand faces kiss example boy black seven spades girl red seven hearts two kiss players matching cards kissed collect cards shuffle cards play keep playing everyone gotten couple kisses different people'\n",
      " 'people psoriatic arthritis can safely develop physical activity regimen based needs however complicating factor heart disease arrhythmia similar condition consult doctor adopting new physical activity regimen re taking prescription medication also ask doctor can stay active psoriatic arthritis attempt exercises exceed capabilities pushing hard result injury elderly infirm addition psoriatic arthritis stick moderate intensity exercises staying active psoriatic arthritis rest needed irritate painful joints instance elbow hurts due psoriatic arthritis try running walking instead lifting weights might like every activity trying various sorts activities might discover new ones relieve psoriatic arthritis trying variety different activities helps prevent boredom predictable physical activity regimen warmup cooldown exercises precede conclude respectively period physical activity work physical therapist identify beneficial warmup cooldown exercises might include extending arms straight sides rotating concentric circles forward backward lunges pushups sit ups reaching touching toes holding position ten seconds swinging legs forward back side side manner pendulum'\n",
      " 'send articles incorporation secretary state current filing fee public disclosure nonprofit also needs filed secretary state include information name location description nonprofit will also need list information board directors officers fill statement online businessfilings sos ca gov will need pay filing fee step well file updated statement information secretary state every years registry charitable trusts within attorney general office needs form filled nonprofit accepts donations'] 61477\n",
      "['always keep lights turn music sound get feeling ghost coming soon going fight boss ghost turn sound prepared break atmosphere completely listen comical person irrelevant cheerful song play either make laugh distract enough least make game appear less scary companion play someone willing'\n",
      " 'open iphone settings scroll tap notes tap default account tap account want set default open iphone settings scroll tap notes tap accounts tap add account tap type account want add type login information account tap next enable notes switch tap save'\n",
      " 'top costumes especially preparing show buy real dance bag every lesson keep small book bag can read wash dance clothes soon possible can one drawer dedicated entirely dance stuff clean dance shoes needed ask teacher attempting prepared'\n",
      " ...\n",
      " 'separate deck playing cards black red decks distribute playing cards ask everyone reveal cards pair matches collect cards play'\n",
      " 'consult doctor choose activities accommodate try variety activities plan warming cooling'\n",
      " 'file articles incorporation state california prepare statement information register attorney general california'] 36212\n",
      "['wait add grilling grid ready cook food get lighter fluid self added enough lighter fluid charcoal briquettes slightly shiny using long match long lighter light charcoal briquettes grill ready charcoal grilling mostly covered ash glow red general plan take minutes charcoal briquettes hot enough charcoal grilling regardless type food briquettes spread bottom grill slightly wider width food allow even cooking use tongs spatula spread charcoal thinner meat spread charcoal evenly bottom grill thicker meat make charcoal higher one side will start cooking meat side charcoal outside cooked preference finish cooking meat side less charcoal using grilling gloves carefully put grilling grate place exact steps cook food will depend cooking', 'clean ash dirt grill beginning remove grill grate grill open vents bottom grill arrange charcoal briquettes pyramid bottom grill add lighter fluid charcoal briquettes light charcoal spread charcoal briquettes bottom grill according type food cooking replace grilling grate cook food finished']\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData(x_train, y_train , False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf84bf37-695a-40f8-904f-b2f92bc3527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "936466eb-0797-418c-8f32-0e1e3619ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63b69a2f-8cd3-4fe2-91b8-28949db8166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea6f53a8-bb06-484b-8bfc-390c213f99da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dcfbcd8-887a-4e1a-8402-2dea34b87d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3490d0b-7751-46d8-9215-41d55e5f70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84a9b62-26cc-481f-89ce-748434840657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81c258a0-33ff-4480-aff0-ffe77970ba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74d3c2ad-cdd4-461d-8230-a8e440165306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    print(\"Training....\")\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(84000, n_iters + 1):\n",
    "        if iter% 1000 == 0:\n",
    "            print(iter,\"/\",n_iters + 1)\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        if(input_length > 150):\n",
    "          #print(input_length)\n",
    "          continue\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "            torch.save(encoder, \"encoder\"+str(iter)+\".pt\")\n",
    "            torch.save(decoder, \"decoder\"+str(iter)+\".pt\")\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aabd258-65bb-4767-a1c6-40da53021c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "379b6386-1be0-4785-9abc-adbf43620a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = torch.load(\"encoder84000.pt\")\n",
    "dec = torch.load(\"decoder84000.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf3ec2-720e-4cbf-9e71-e5c1b0377444",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 300\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b8efeaa-f00c-4904-a8d6-316895e8a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "84000 / 150001\n",
      "0m 12s (- 0m 9s) (84000 56%) 0.0092\n",
      "85000 / 150001\n",
      "2m 9s (- 1m 38s) (85000 56%) 5.2804\n",
      "86000 / 150001\n",
      "4m 7s (- 3m 4s) (86000 57%) 5.3114\n",
      "87000 / 150001\n",
      "6m 6s (- 4m 25s) (87000 57%) 5.2891\n",
      "88000 / 150001\n",
      "8m 3s (- 5m 40s) (88000 58%) 5.3175\n",
      "89000 / 150001\n",
      "9m 59s (- 6m 51s) (89000 59%) 5.3471\n",
      "90000 / 150001\n",
      "11m 59s (- 7m 59s) (90000 60%) 5.1097\n",
      "91000 / 150001\n",
      "13m 56s (- 9m 2s) (91000 60%) 5.2207\n",
      "92000 / 150001\n",
      "15m 55s (- 10m 2s) (92000 61%) 5.1956\n",
      "93000 / 150001\n",
      "17m 56s (- 10m 59s) (93000 62%) 5.2913\n",
      "94000 / 150001\n",
      "19m 56s (- 11m 53s) (94000 62%) 5.1533\n",
      "95000 / 150001\n",
      "21m 55s (- 12m 41s) (95000 63%) 5.4039\n",
      "96000 / 150001\n",
      "23m 56s (- 13m 28s) (96000 64%) 5.2610\n",
      "97000 / 150001\n",
      "25m 54s (- 14m 9s) (97000 64%) 5.3113\n",
      "98000 / 150001\n",
      "27m 56s (- 14m 49s) (98000 65%) 5.3845\n",
      "99000 / 150001\n",
      "29m 54s (- 15m 24s) (99000 66%) 5.2648\n",
      "100000 / 150001\n",
      "31m 52s (- 15m 56s) (100000 66%) 5.2637\n",
      "101000 / 150001\n",
      "33m 52s (- 16m 26s) (101000 67%) 5.2289\n",
      "102000 / 150001\n",
      "35m 51s (- 16m 52s) (102000 68%) 5.3039\n",
      "103000 / 150001\n",
      "37m 49s (- 17m 15s) (103000 68%) 5.1600\n",
      "104000 / 150001\n",
      "39m 47s (- 17m 35s) (104000 69%) 5.2587\n",
      "105000 / 150001\n",
      "41m 47s (- 17m 54s) (105000 70%) 5.2673\n",
      "106000 / 150001\n",
      "43m 48s (- 18m 11s) (106000 70%) 5.4442\n",
      "107000 / 150001\n",
      "45m 48s (- 18m 24s) (107000 71%) 5.3595\n",
      "108000 / 150001\n",
      "109000 / 150001\n",
      "49m 42s (- 18m 41s) (109000 72%) 10.4478\n",
      "110000 / 150001\n",
      "51m 39s (- 18m 47s) (110000 73%) 5.1694\n",
      "111000 / 150001\n",
      "53m 34s (- 18m 49s) (111000 74%) 5.1456\n",
      "112000 / 150001\n",
      "55m 33s (- 18m 50s) (112000 74%) 5.2187\n",
      "113000 / 150001\n",
      "57m 30s (- 18m 49s) (113000 75%) 5.2594\n",
      "114000 / 150001\n",
      "59m 28s (- 18m 46s) (114000 76%) 5.2848\n",
      "115000 / 150001\n",
      "61m 28s (- 18m 42s) (115000 76%) 5.2612\n",
      "116000 / 150001\n",
      "63m 24s (- 18m 35s) (116000 77%) 5.1098\n",
      "117000 / 150001\n",
      "65m 22s (- 18m 26s) (117000 78%) 5.2259\n",
      "118000 / 150001\n",
      "67m 22s (- 18m 16s) (118000 78%) 5.3834\n",
      "119000 / 150001\n",
      "69m 21s (- 18m 4s) (119000 79%) 5.2763\n",
      "120000 / 150001\n",
      "71m 20s (- 17m 50s) (120000 80%) 5.2101\n",
      "121000 / 150001\n",
      "73m 20s (- 17m 34s) (121000 80%) 5.2700\n",
      "122000 / 150001\n",
      "75m 21s (- 17m 17s) (122000 81%) 5.2863\n",
      "123000 / 150001\n",
      "77m 21s (- 16m 58s) (123000 82%) 5.2352\n",
      "124000 / 150001\n",
      "79m 22s (- 16m 38s) (124000 82%) 5.2454\n",
      "125000 / 150001\n",
      "81m 21s (- 16m 16s) (125000 83%) 5.2724\n",
      "126000 / 150001\n",
      "83m 17s (- 15m 51s) (126000 84%) 5.1737\n",
      "127000 / 150001\n",
      "85m 13s (- 15m 25s) (127000 84%) 5.0760\n",
      "128000 / 150001\n",
      "87m 13s (- 14m 59s) (128000 85%) 5.1915\n",
      "129000 / 150001\n",
      "89m 13s (- 14m 31s) (129000 86%) 5.3439\n",
      "130000 / 150001\n",
      "91m 10s (- 14m 1s) (130000 86%) 5.1309\n",
      "131000 / 150001\n",
      "93m 7s (- 13m 30s) (131000 87%) 5.1704\n",
      "132000 / 150001\n",
      "95m 5s (- 12m 57s) (132000 88%) 5.1827\n",
      "133000 / 150001\n",
      "97m 3s (- 12m 24s) (133000 88%) 5.1950\n",
      "134000 / 150001\n",
      "99m 4s (- 11m 49s) (134000 89%) 5.2481\n",
      "135000 / 150001\n",
      "101m 1s (- 11m 13s) (135000 90%) 5.2091\n",
      "136000 / 150001\n",
      "103m 2s (- 10m 36s) (136000 90%) 5.2097\n",
      "137000 / 150001\n",
      "105m 2s (- 9m 58s) (137000 91%) 5.2180\n",
      "138000 / 150001\n",
      "107m 3s (- 9m 18s) (138000 92%) 5.1753\n",
      "139000 / 150001\n",
      "109m 1s (- 8m 37s) (139000 92%) 5.2866\n",
      "140000 / 150001\n",
      "110m 58s (- 7m 55s) (140000 93%) 5.0542\n",
      "141000 / 150001\n",
      "112m 57s (- 7m 12s) (141000 94%) 5.2769\n",
      "142000 / 150001\n",
      "114m 56s (- 6m 28s) (142000 94%) 5.1707\n",
      "143000 / 150001\n",
      "116m 55s (- 5m 43s) (143000 95%) 5.1976\n",
      "144000 / 150001\n",
      "118m 51s (- 4m 57s) (144000 96%) 5.1716\n",
      "145000 / 150001\n",
      "120m 51s (- 4m 10s) (145000 96%) 5.2755\n",
      "146000 / 150001\n",
      "122m 48s (- 3m 21s) (146000 97%) 5.1129\n",
      "147000 / 150001\n",
      "124m 48s (- 2m 32s) (147000 98%) 5.2718\n",
      "148000 / 150001\n",
      "126m 50s (- 1m 42s) (148000 98%) 5.2529\n",
      "149000 / 150001\n",
      "128m 47s (- 0m 51s) (149000 99%) 5.1920\n",
      "150000 / 150001\n",
      "130m 46s (- 0m 0s) (150000 100%) 5.2532\n"
     ]
    }
   ],
   "source": [
    "trainIters(enc, dec, 150000, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd810f68-6a35-4323-bb11-6b04ecf52c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoder1, \"encoder.pt\")\n",
    "torch.save(attn_decoder1, \"decoder.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68cb90a3-ce06-47b1-982e-c1ec749076d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner_metaphor(text,num):\n",
    "  str = re.sub(\"[^a-zA-Z.]\", \" \", text)  \n",
    "  #str = re.sub(\"div\", \" \", text)  \n",
    "  tokens=str.split()\n",
    "    \n",
    "  long_words=[]\n",
    "  for i in tokens:\n",
    "      if(i==\"div\"):\n",
    "          continue\n",
    "      if len(i)>1:          #removing short words\n",
    "          long_words.append(i)\n",
    "  return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce97550-7877-441a-882a-25e54d51d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaphor_python import Metaphor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2dd2e09-134b-486d-b0b4-978cb5eaf82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaphor = Metaphor(\"23c84584-6ebc-4159-9fa1-b3cebf43a8a1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f449686-80e7-4b54-84d3-001c712330d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = metaphor.search(\"iphone 12 reviews\", num_results = 10, use_autoprompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1dc8857-254a-45f8-a25b-accd59360291",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=response.get_contents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9108d-5440-47e1-a796-19177c32ca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "ress = res.contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4ecc55c-13d8-4c54-aa95-9f795e960b39",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Request failed with status code 429. Message: {\"error\":\"API key usage limit reached: 1009/1000\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m index \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandrange(\u001b[38;5;241m20000\u001b[39m)\n\u001b[1;32m      5\u001b[0m title \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m][index]\n\u001b[0;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmetaphor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_autoprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m t \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget_contents()\u001b[38;5;241m.\u001b[39mcontents[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mextract\n\u001b[1;32m      8\u001b[0m clean\u001b[38;5;241m=\u001b[39mtext_cleaner(t,\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/metaphor_python/api.py:148\u001b[0m, in \u001b[0;36mMetaphor.search\u001b[0;34m(self, query, num_results, include_domains, exclude_domains, start_crawl_date, end_crawl_date, start_published_date, end_published_date, use_autoprompt, type)\u001b[0m\n\u001b[1;32m    146\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/search\u001b[39m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m=\u001b[39mrequest, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest failed with status code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m results \u001b[38;5;241m=\u001b[39m [Result(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_snake_case(result)) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    150\u001b[0m autoprompt_string \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautopromptString\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautopromptString\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mException\u001b[0m: Request failed with status code 429. Message: {\"error\":\"API key usage limit reached: 1009/1000\"}"
     ]
    }
   ],
   "source": [
    "import random\n",
    "clean_text = []\n",
    "for i in range(500): #maximum requests\n",
    "    index = random.randrange(20000)\n",
    "    title = df[\"title\"][index]\n",
    "    response = metaphor.search(title, num_results = 10, use_autoprompt=True,)\n",
    "    t = response.get_contents().contents[1].extract\n",
    "    clean=text_cleaner(t,0)\n",
    "    clean_text.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e6c880e4-b6e9-4914-a2a3-e30706d52028",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_GRU = pd.DataFrame()\n",
    "pred_df_GRU[\"text\"] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd12524-48fe-48a1-a3eb-c18540e10dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_summary=[]\n",
    "for i in range(len(pred_df_GRU[\"text\"])):\n",
    "    output_words, attentions = evaluate(enc, dec,pred_df_GRU[\"text\"][i])\n",
    "    pred_summary+=[output_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "472585d6-749a-4e16-b229-cad29a3cb700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As partner to woman in labor your biggest role...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>href https www.trustpilot.com review yeastinfe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first day of kindergarten is busy busy bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strong Learn How to Get Better at Soccer with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article How to relieve tattoo pain or how to m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  As partner to woman in labor your biggest role...\n",
       "1  href https www.trustpilot.com review yeastinfe...\n",
       "2  The first day of kindergarten is busy busy bus...\n",
       "3  strong Learn How to Get Better at Soccer with ...\n",
       "4  article How to relieve tattoo pain or how to m..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df_GRU[\"pred_summary\"] = pred_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d09847b1-476f-4ac7-ba06-8128c68e7106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pred_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As partner to woman in labor your biggest role...</td>\n",
       "      <td>As partner to woman in labor your biggest role...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>href https www.trustpilot.com review yeastinfe...</td>\n",
       "      <td>em What is the best yeast infection prevention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The first day of kindergarten is busy busy bus...</td>\n",
       "      <td>The first day of kindergarten is busy busy bus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strong Learn How to Get Better at Soccer with ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article How to relieve tattoo pain or how to m...</td>\n",
       "      <td>article How to relieve tattoo pain or how to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>DIY Cat Condo href http .bp.blogspot.com lZoVJ...</td>\n",
       "      <td>DIY Cat Condo href http .bp.blogspot.com lZoVJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Vagina is one of the organs in woman body char...</td>\n",
       "      <td>Vagina is one of the organs in woman body char...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>main section header header article figure figc...</td>\n",
       "      <td>main section header header article figure figc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>So you want to know how to tell if guy likes y...</td>\n",
       "      <td>So you want to know how to tell if guy likes y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>For your bad ass girl gang look. We loved how ...</td>\n",
       "      <td>We loved how Arabelle did Nicki eyes in the ph...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   As partner to woman in labor your biggest role...   \n",
       "1   href https www.trustpilot.com review yeastinfe...   \n",
       "2   The first day of kindergarten is busy busy bus...   \n",
       "3   strong Learn How to Get Better at Soccer with ...   \n",
       "4   article How to relieve tattoo pain or how to m...   \n",
       "..                                                ...   \n",
       "70  DIY Cat Condo href http .bp.blogspot.com lZoVJ...   \n",
       "71  Vagina is one of the organs in woman body char...   \n",
       "72  main section header header article figure figc...   \n",
       "73  So you want to know how to tell if guy likes y...   \n",
       "74  For your bad ass girl gang look. We loved how ...   \n",
       "\n",
       "                                         pred_summary  \n",
       "0   As partner to woman in labor your biggest role...  \n",
       "1   em What is the best yeast infection prevention...  \n",
       "2   The first day of kindergarten is busy busy bus...  \n",
       "3                                                 NaN  \n",
       "4   article How to relieve tattoo pain or how to m...  \n",
       "..                                                ...  \n",
       "70  DIY Cat Condo href http .bp.blogspot.com lZoVJ...  \n",
       "71  Vagina is one of the organs in woman body char...  \n",
       "72  main section header header article figure figc...  \n",
       "73  So you want to know how to tell if guy likes y...  \n",
       "74  We loved how Arabelle did Nicki eyes in the ph...  \n",
       "\n",
       "[75 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "51810a8b-b889-4607-8980-cbd90e52c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_GRU.to_csv(\"pred_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "44992d57-a7c8-4cdf-b669-ad53aff1f1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_table = pred_df_GRU.to_html(index=False)\n",
    "\n",
    "# Save the HTML table to a file\n",
    "with open('my-table.html', 'w') as f:\n",
    "    f.write(html_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0e617-a04c-4866-9749-ae0477d6fecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
